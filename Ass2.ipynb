{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63ZCVyKHcvlT",
        "outputId": "2372a26f-eaf7-4184-9ddb-d6824238f1b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z87ZslYXmmr",
        "outputId": "175eacbc-9dec-451a-98cc-d3fee8e244db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "3y6H41CjXpg4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CdD0mzmgXFNc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "import math\n",
        "import string\n",
        "from collections import Counter\n",
        "from scipy.sparse import csr_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    preprocessed_data = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lower case\n",
        "    preprocessed_data = preprocessed_data.lower()\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(preprocessed_data)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Function to generate a random sentence using GPT-2 model\n",
        "def generate_random_sentence(prompt,length):\n",
        "    generator = pipeline('text-generation', model='gpt2')\n",
        "    generated_text = generator(prompt, max_length=50)[0]['generated_text']\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "\n",
        "def generate_preprocessed_document( prompt,length):\n",
        "    document =generate_random_sentence(prompt,length)\n",
        "    return preprocess_text(document)"
      ],
      "metadata": {
        "id": "ZS1FEjz2YV7j"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length = 100\n",
        "prompt1 = \"Artificial intelligence (AI) is\"\n",
        "prompt2 = \"Data science is\"\n",
        "preprocessed_document1 = generate_preprocessed_document( prompt1 , length)\n",
        "preprocessed_document2= generate_preprocessed_document(prompt2 , length)\n"
      ],
      "metadata": {
        "id": "KHI2q0QbYV95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f4d354-3879-4a5b-d990-0cb7990dbf05"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed_document1)\n",
        "print(len(preprocessed_document1))\n",
        "unique_words_document1 = set(preprocessed_document1)\n",
        "unique_words_document2 = set(preprocessed_document2)\n",
        "print(\"Unique words:\", unique_words_document1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3cR9484dsJj",
        "outputId": "b5ea1787-07bf-4064-c3ad-d51f338b6ac6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'ai', 'becoming', 'driving', 'factor', 'generation', 'technology', 'could', 'someday', 'drive', 'robot', 'point', 'job', 'job', 'food', 'delivery', 'even', 'company', 'computer', 'based', 'artificial', 'intelligence', 'could', 'become']\n",
            "25\n",
            "Unique words: {'delivery', 'becoming', 'factor', 'even', 'ai', 'drive', 'robot', 'someday', 'become', 'food', 'intelligence', 'could', 'generation', 'company', 'driving', 'technology', 'point', 'computer', 'artificial', 'based', 'job'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(doc):\n",
        "    tf = {}\n",
        "    word_counts = Counter(doc)\n",
        "    total_words = len(doc)\n",
        "    for word, count in word_counts.items():\n",
        "        tf[word] = count / total_words\n",
        "    return tf\n",
        "\n",
        "\n",
        "import math\n",
        "import builtins\n",
        "\n",
        "import math\n",
        "\n",
        "def inverse_document_frequency(documents):\n",
        "    idf = {}\n",
        "    total_documents = len(documents)\n",
        "    all_words = set([word for doc in documents for word in doc])\n",
        "    for word in all_words:\n",
        "        document_count = sum([1 for doc in documents if word in doc])\n",
        "        idf[word] = math.log(total_documents / (1 + document_count))\n",
        "    return idf\n",
        "\n",
        "\n",
        "def tfidf(documents):\n",
        "    tfidf_matrix = []\n",
        "    idf = inverse_document_frequency(documents)\n",
        "    for doc in documents:\n",
        "        tf = term_frequency(doc)\n",
        "        tfidf_vector = {}\n",
        "        for word in tf:\n",
        "            tfidf_vector[word] = tf[word] * idf[word]\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "    return tfidf_matrix\n",
        "\n",
        "\n",
        "\n",
        "def print_tfIdf(tfidf_matrix, documents):\n",
        "    for i, doc_tfidf in enumerate(tfidf_matrix):\n",
        "        print(f\"Document {i + 1}:\")\n",
        "        for word in documents[i].split():\n",
        "            tfidf_value = doc_tfidf.get(word, 0)  # Get TF-IDF value for the word, default to 0 if word not found\n",
        "            print(f\"{word}: {tfidf_value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "W1Xzj1tNg7KC"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preprocessed_docs = [' '.join(preprocessed_document1), ' '.join(preprocessed_document2)]\n",
        "print(term_frequency(preprocessed_document2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_sDSBHPhBoq",
        "outputId": "270e2107-d98f-4ee2-e5be-9a8cfb39b5ca"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'data': 0.037037037037037035, 'science': 0.037037037037037035, 'great': 0.07407407407407407, 'way': 0.037037037037037035, 'think': 0.037037037037037035, 'world': 0.037037037037037035, 'without': 0.037037037037037035, 'making': 0.037037037037037035, 'mistake': 0.037037037037037035, 'focusing': 0.037037037037037035, 'much': 0.037037037037037035, 'time': 0.037037037037037035, 'pesky': 0.037037037037037035, 'thing': 0.037037037037037035, 'possible': 0.037037037037037035, 'order': 0.037037037037037035, 'make': 0.037037037037037035, 'quick': 0.037037037037037035, 'trip': 0.037037037037037035, 'video': 0.07407407407407407, 'youll': 0.037037037037037035, 'need': 0.037037037037037035, 'create': 0.037037037037037035, 'simple': 0.037037037037037035, 'app': 0.037037037037037035}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf(all_preprocessed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgtL-tklanUA",
        "outputId": "35e010d5-0448-4fef-bc81-c253638099d8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'a': -0.021228539691526934,\n",
              "  'r': -0.01910568572237424,\n",
              "  't': -0.021228539691526934,\n",
              "  'i': -0.03821137144474848,\n",
              "  'f': -0.008491415876610774,\n",
              "  'c': -0.02547424762983232,\n",
              "  'l': -0.021228539691526934,\n",
              "  ' ': -0.05094849525966464,\n",
              "  'n': -0.02547424762983232,\n",
              "  'e': -0.04245707938305387,\n",
              "  'g': -0.01273712381491616,\n",
              "  'b': -0.01273712381491616,\n",
              "  'o': -0.03821137144474848,\n",
              "  'm': -0.010614269845763467,\n",
              "  'd': -0.016982831753221548,\n",
              "  'v': -0.008491415876610774,\n",
              "  'h': -0.0021228539691526935,\n",
              "  'y': -0.008491415876610774,\n",
              "  'u': -0.00636856190745808,\n",
              "  's': -0.004245707938305387,\n",
              "  'p': -0.00636856190745808,\n",
              "  'j': 0.0},\n",
              " {'d': -0.014655365353307147,\n",
              "  'a': -0.02442560892217858,\n",
              "  't': -0.02686816981439644,\n",
              "  ' ': -0.0635065831976643,\n",
              "  's': -0.017097926245525006,\n",
              "  'c': -0.014655365353307147,\n",
              "  'i': -0.03419585249105001,\n",
              "  'e': -0.04152353516770359,\n",
              "  'n': -0.014655365353307147,\n",
              "  'g': -0.01221280446108929,\n",
              "  'r': -0.017097926245525006,\n",
              "  'w': 0.0,\n",
              "  'y': -0.007327682676653574,\n",
              "  'h': -0.009770243568871432,\n",
              "  'k': 0.0,\n",
              "  'o': -0.019540487137742864,\n",
              "  'l': -0.01221280446108929,\n",
              "  'u': -0.01221280446108929,\n",
              "  'm': -0.014655365353307147,\n",
              "  'f': -0.002442560892217858,\n",
              "  'p': -0.014655365353307147,\n",
              "  'b': -0.002442560892217858,\n",
              "  'q': 0.0,\n",
              "  'v': -0.004885121784435716}]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_document_frequency(all_preprocessed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p129nG2mQOy",
        "outputId": "10e2ad2c-b409-41ff-8bcd-8741f46d53cf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'n': -0.40546510810816444,\n",
              " 'q': 0.0,\n",
              " 'b': -0.40546510810816444,\n",
              " 'p': -0.40546510810816444,\n",
              " 'a': -0.40546510810816444,\n",
              " 'e': -0.40546510810816444,\n",
              " 'i': -0.40546510810816444,\n",
              " 'm': -0.40546510810816444,\n",
              " 'r': -0.40546510810816444,\n",
              " 'j': 0.0,\n",
              " ' ': -0.40546510810816444,\n",
              " 'l': -0.40546510810816444,\n",
              " 'h': -0.40546510810816444,\n",
              " 'f': -0.40546510810816444,\n",
              " 'g': -0.40546510810816444,\n",
              " 'k': 0.0,\n",
              " 'o': -0.40546510810816444,\n",
              " 'v': -0.40546510810816444,\n",
              " 'd': -0.40546510810816444,\n",
              " 'w': 0.0,\n",
              " 'y': -0.40546510810816444,\n",
              " 't': -0.40546510810816444,\n",
              " 'u': -0.40546510810816444,\n",
              " 's': -0.40546510810816444,\n",
              " 'c': -0.40546510810816444}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}